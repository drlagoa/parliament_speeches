{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bb3d3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis module tokenizes text and divides the dataset into categories for further analysis.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This module tokenizes text and divides the dataset into categories (political parties, legislative sessions) for further analysis.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "567415a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "# Regular Expression library\n",
    "import re\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import matplotlib.pyplot as mplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60d6a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing important packages\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1781ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:tese_data\\save\n"
     ]
    }
   ],
   "source": [
    "directory_save = os.path.join('D:', 'tese_data', 'save')\n",
    "print(directory_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36b4bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file\n",
    "\n",
    "file_path = os.path.join(directory_save, 'clean_dataframe.json')\n",
    "\n",
    "speeches = pd.read_json(file_path, orient = \"records\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b01ac1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 columns and 221036 rows.\n"
     ]
    }
   ],
   "source": [
    "# Some information on the dimension of the panda dataframe.\n",
    "\n",
    "speeches.shape\n",
    "print('There are {} columns and {} rows.'.format(speeches.shape[1], speeches.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2dd2e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8357ec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dataframe = speeches\n",
    "clean_parties = ['BE', 'PSD+CDS-PP', 'Os Verdes', 'PCP', 'PS', 'PSD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48396d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bae52124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, I create the word lists per party. That is how the functions are structured, but easily adjustable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b155192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by defining the relevant stopwords.\n",
    "\n",
    "stop_words = stopwords.words('portuguese')\n",
    "\n",
    "common_parl_words = ['sr', 'governo', 'lei', 'presidente', 'srs',\n",
    "                     'regime', 'aplausos', 'deputados', 'deputado',\n",
    "                     'ministro', 'estado', 'pais', 'srª', 'partido',\n",
    "                     'portugal', 'sras', 'portugueses', 'proposta',\n",
    "                     'propostas', 'medida', 'medidas', 'deputada',\n",
    "                     'ministra', 'assembleia', 'republica', 'socialista',\n",
    "                     'país', 'voto', 'número', 'parlamentar',\n",
    "                     'républica', 'senhores', 'senhor',\n",
    "                     'senhora', 'série', 'falar', 'chega',\n",
    "                     'grupo', 'intervenção', 'votação', 'joão',\n",
    "                     'pedro', 'declaração', 'questão', 'parlamento',\n",
    "                     'antónio', 'projeto', 'josé', 'iniciativa',\n",
    "                     'madeira', 'açores', 'resolução', 'partidos',\n",
    "                     'secretário', 'palavra', 'mesa', 'caros',\n",
    "                     'discussão', 'nº', 'bancada', 'tempo',\n",
    "                     'dia', 'ponto', 'resposta', 'liberal', 'pergunta',\n",
    "                     'colegas', 'caras', 'informar', 'questões', 'responder',\n",
    "                     'politica', 'protestos', 'bloco', 'esquerda']\n",
    "\n",
    "common_pt_words = ['nao', 'tambem', 'porque', 'sao', 'ha', 'ja',\n",
    "                   'sobre', 'aqui', 'todos', 'hoje', 'fazer', 'anos',\n",
    "                   'primeiro', 'ter', 'dizer', 'so', 'ainda', 'ate',\n",
    "                   'serie', 'pode', 'questao', 'ano', 'vai', 'facto',\n",
    "                   'materia', 'debate', 'quer', 'portanto', 'vez', 'assim',\n",
    "                   'deste', 'desta', 'parte', 'sempre', 'nesta', 'qualquer',\n",
    "                   'tudo', 'apenas', 'situacao', 'sentido', 'deve',\n",
    "                   'todas', 'importante', 'momento', 'onde',\n",
    "                   'dar', 'contra', 'quanto', 'têm',\n",
    "                   'nada', 'alias', 'outros', 'ver', 'agora',\n",
    "                   'bem', 'coisa', 'disse', 'realidade', 'vezes',\n",
    "                   'dia', 'nunca', 'fez', 'meses', 'sabe', 'feito',\n",
    "                   'preciso', 'tão', 'aliás', 'queria', 'caso',\n",
    "                   'relativamente', 'vou', 'quero', 'cada', 'exemplo',\n",
    "                   'sabemos', 'há', 'já', 'até', 'faz', 'novo',\n",
    "                   'vão', 'nomeadamente', 'toda', 'verdade',\n",
    "                   'tipo', 'podemos', 'mesmo', 'mesma', 'relação',\n",
    "                   'acerca', 'saber', 'devemos', 'outro', 'gostaria',\n",
    "                   'então', 'podem', 'nome', 'matéria', 'sim', 'não',\n",
    "                   'estao', 'neste', 'forma', 'diz', 'outras']\n",
    "\n",
    "parties = ['be', 'cds', 'pp', 'ch', 'il', 'l',\n",
    "            'livre', 'verdes', 'pan', 'pcp', 'ps', 'psd']\n",
    "\n",
    "stop_words.extend(common_parl_words)\n",
    "stop_words.extend(common_pt_words)\n",
    "stop_words.extend(parties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee55b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aafabf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1131883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define how to tokenize the text and remove stopwords.\n",
    "    \n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        tokens = nltk.word_tokenize(sentence, language='portuguese')\n",
    "        tokens = [token.lower() for token in tokens if token.isalpha()] # remove punctuation and numbers\n",
    "\n",
    "        yield(tokens)\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5e6d4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function simply consolidates the two above, and introduces criteria for party.\n",
    "\n",
    "def tokenize(dataframe, party=None, year=None, session=None):\n",
    "    '''Prepares and returns the data_words list based on party, year, and optional session.'''\n",
    "    \n",
    "    if party is None:\n",
    "        party = \"all\"\n",
    "    \n",
    "    if party == \"all\":\n",
    "        if year is not None:\n",
    "            filtered_data = dataframe[dataframe['Date'].dt.year == year]\n",
    "        else:\n",
    "            filtered_data = dataframe\n",
    "        \n",
    "        if session is not None:\n",
    "            filtered_data = filtered_data[filtered_data['Session'] == session]\n",
    "        \n",
    "        data = filtered_data.Intervention_processed.values.tolist()\n",
    "    elif party in clean_parties:\n",
    "        if year is not None:\n",
    "            filtered_data = dataframe[(dataframe['Party'] == party) & (dataframe['Date'].dt.year == year)]\n",
    "        else:\n",
    "            filtered_data = dataframe[dataframe['Party'] == party]\n",
    "        \n",
    "        if session is not None:\n",
    "            filtered_data = filtered_data[filtered_data['Session'] == session]\n",
    "        \n",
    "        data = filtered_data.Intervention_processed.values.tolist()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid party specified\")\n",
    "    \n",
    "    data_words = list(sent_to_words(data))\n",
    "    \n",
    "    # Remove stop words\n",
    "    data_words = remove_stopwords(data_words)\n",
    "    \n",
    "    if year is not None and session is not None:\n",
    "        save_list(data_words, party=party, year=year, session=session)\n",
    "    elif year is not None:\n",
    "        save_list(data_words, party=party, year=year)\n",
    "    elif session is not None:\n",
    "        save_list(data_words, party=party, session=session)\n",
    "    \n",
    "    return data_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6f7fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(lst, party=None, year=None, session=None):\n",
    "    '''Saves list of words for future use.'''\n",
    "    \n",
    "    directory_save_panda = os.path.join('D:', 'tese_data', 'save', 'panda')\n",
    "    \n",
    "    if party is None:\n",
    "        party = \"all\"\n",
    "    \n",
    "    if party == \"all\":\n",
    "        if year is not None:\n",
    "            if session is not None:\n",
    "                st = f'data_words_{year}_{session}.json'\n",
    "            else:\n",
    "                st = f'data_words_{year}.json'\n",
    "        elif session is not None:\n",
    "            st = f'data_words_{session}.json'\n",
    "        else:\n",
    "            st = 'data_words.json'\n",
    "    elif party in clean_parties:\n",
    "        if year is not None:\n",
    "            if session is not None:\n",
    "                st = f'data_words_{party}_{year}_{session}.json'\n",
    "            else:\n",
    "                st = f'data_words_{party}_{year}.json'\n",
    "        elif session is not None:\n",
    "            st = f'data_words_{party}_{session}.json'\n",
    "        else:\n",
    "            st = f'data_words_{party}.json'\n",
    "    else:\n",
    "        raise ValueError(\"Invalid party specified\")\n",
    "    \n",
    "    file_name = os.path.join(directory_save_panda, st)\n",
    "\n",
    "    with open(file_name, 'w') as outfile:\n",
    "        json.dump(lst, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1053abdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fafa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = tokenize(clean_dataframe)\n",
    "save_list(data_words)\n",
    "\n",
    "# Total count of words:\n",
    "compiled = []\n",
    "\n",
    "for lst in data_words:\n",
    "    for i in lst:\n",
    "        compiled.append(i)\n",
    "\n",
    "print(len(compiled))\n",
    "\n",
    "print(\"Total count of words is: \", compiled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e254cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for party in clean_parties:\n",
    "    \n",
    "    data_words = tokenize(clean_dataframe, party=party)\n",
    "    save_list(data_words, party=party)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace8455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8b59ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5ffc10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2005, 2020):\n",
    "    \n",
    "    data_words = tokenize(clean_dataframe, year=year)\n",
    "    save_list(data_words, year=year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9506afa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for session in range(10, 14):\n",
    "    \n",
    "    data_words = tokenize(clean_dataframe, session=session)\n",
    "    save_list(data_words, session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec5f2dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for party in clean_parties:\n",
    "    for session in range(10,14):\n",
    "        \n",
    "        data_words = tokenize(clean_dataframe, party = party, session = session)\n",
    "        save_list(data_words, party=party, session = session)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
